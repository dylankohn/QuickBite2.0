# Server Configuration
PORT=5000

# Ollama Service Configuration
# For local development, use: http://localhost:11434
# For production, use your hosted Ollama service URL
OLLAMA_URL=http://localhost:11434

# Model Configuration
# Available models: llama3.2:3b, llama2:7b, etc.
MODEL_NAME=llama3.2:3b

# Environment
NODE_ENV=development 